{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN family.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Syilun/TibameAI04/blob/master/RNN_family.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MQTJpgXIZTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyuU_zLAQTZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sample: 1\n",
        "# Sequence Length: 5\n",
        "# feature dimension: 8\n",
        "# 如何決定input & output shape\n",
        "# batchsize = sample\n",
        "\n",
        "inputs = tf.random.normal([1, 5, 8])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atmpBvPyLWCJ",
        "colab_type": "text"
      },
      "source": [
        "# tf.keras.layers.LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad4M9eEwIddc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "49f7f817-adc4-411b-f613-4a4572157a6c"
      },
      "source": [
        "output = layers.LSTM(units=3)(inputs)\n",
        "print(output.shape)\n",
        "\n",
        "# units\n",
        "# 就像 dense layer 的神經元數目\n",
        "# 決定output向量長度是多少\n",
        "# 每一個時間點向量長度8，總共餵五次\n",
        "# output.shape = 1,3 \n",
        "# 1=batchsize, 3=output feature dimension\n",
        "# 得到的結果是 1 個向量長度3的output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t8D3p7dJ0vg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "f81f13ab-3446-4992-dc5c-7591c8a4653d"
      },
      "source": [
        "whole_seq_output, final_hiden_state, final_cell_state = layers.LSTM(3, return_sequences=True, return_state=True)(inputs)\n",
        "\n",
        "# return_sequences 意思是 output其實都會有結果，預設為False，當設為True，會得到所有output \n",
        "# return_state 預設也是False，當設為True，會給你多兩個state\n",
        "# final_hiden_state 也就是(1,3)那個output\n",
        "# final_cell_state 意思是C_t也會丟給你，它的形狀也是(1,3)\n",
        "\n",
        "\n",
        "print('whole_seq_output: ', whole_seq_output.shape, whole_seq_output)\n",
        "print('final_hiden_state (h): ', final_hiden_state.shape, final_hiden_state)\n",
        "print('final_cell_state (c): ', final_cell_state.shape, final_cell_state)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "whole_seq_output:  (1, 5, 3) tf.Tensor(\n",
            "[[[ 0.01134522  0.0120947   0.24948213]\n",
            "  [ 0.11279607 -0.05773085 -0.04871291]\n",
            "  [ 0.0489895   0.16063593  0.01779252]\n",
            "  [ 0.36228287  0.21084888  0.38239726]\n",
            "  [ 0.04803879  0.30942777  0.22481783]]], shape=(1, 5, 3), dtype=float32)\n",
            "final_hiden_state (h):  (1, 3) tf.Tensor([[0.04803879 0.30942777 0.22481783]], shape=(1, 3), dtype=float32)\n",
            "final_cell_state (c):  (1, 3) tf.Tensor([[0.17377621 0.38481298 0.31572753]], shape=(1, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf8aLad2LX9w",
        "colab_type": "text"
      },
      "source": [
        "# tf.keras.layers.GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xxagyy9LYt_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2ec2e748-cb22-4965-ae46-71af1fd1b719"
      },
      "source": [
        "output = layers.GRU(units=3)(inputs)\n",
        "print(output.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2THpViELlO5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "4a005f3e-f84a-4836-c5eb-9d31fb99e68d"
      },
      "source": [
        "whole_sequence_output, final_state = layers.GRU(3, return_sequences=True, return_state=True)(inputs)\n",
        "\n",
        "# 一樣有return_sequences & return_state\n",
        "# 但重點是: GRU沒有 final_cell_state(只有LSTM有)\n",
        "\n",
        "print('whole_seq_output: ', whole_sequence_output.shape, whole_sequence_output)\n",
        "print('final_state (h): ', final_state.shape, final_state)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "whole_seq_output:  (1, 5, 3) tf.Tensor(\n",
            "[[[ 0.1339913  -0.24811737  0.24841756]\n",
            "  [-0.01752811  0.36379862 -0.1139631 ]\n",
            "  [-0.2599124  -0.18762621 -0.8300888 ]\n",
            "  [-0.24967541 -0.44094458 -0.5778853 ]\n",
            "  [ 0.21719603 -0.5088333  -0.32861453]]], shape=(1, 5, 3), dtype=float32)\n",
            "final_state (h):  (1, 3) tf.Tensor([[ 0.21719603 -0.5088333  -0.32861453]], shape=(1, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLcEAbfIM7IT",
        "colab_type": "text"
      },
      "source": [
        "# tf.keras.layers.Bidirectional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aDkLkxeM-XQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "4ce136e0-d2f1-49a2-ff5b-f4b7ca781f88"
      },
      "source": [
        "output = layers.Bidirectional(layers.LSTM(10))(inputs)\n",
        "print(output.shape)\n",
        "\n",
        "# 順向逆向都是每個長度為10的向量的LSTM\n",
        "# 右邊為不修改參數預設RETURN的結果\n",
        "\n",
        "# merge_mode 預設為concat\n",
        "# merge_mode可以選擇要做CONCAT的連接，還是點對點的相加\n",
        "# 如果是CONCAT output為長度20向量\n",
        "# 如果是SUM output為長度10向量\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 20)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-69f68eacd109>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0m順向逆向都是每個長度為10的向量的LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0m右邊為不修改參數預設RETURN的結果\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name '順向逆向都是每個長度為10的向量的LSTM' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMQkgBJ3NYeO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# merge_mode: Sum\n",
        "output = layers.Bidirectional(layers.LSTM(10), merge_mode='sum')(inputs)\n",
        "print(output.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbBbk7e7NqOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# return_sequences: True\n",
        "output = layers.Bidirectional(layers.LSTM(10, return_sequences=True))(inputs)\n",
        "print(output.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3Kh1ZkPNwtA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# return_sequences: True, \n",
        "# 逆向順向都有h & c 的參數，所以return_sequences: True時,有4個\n",
        "output, forward_h, forward_c, backward_h, backward_c = layers.Bidirectional(layers.LSTM(2, return_sequences=True, return_state=True))(inputs)\n",
        "print('output : ', output.shape, output)\n",
        "print('forward_h : ', forward_h.shape, forward_h)\n",
        "print('forward_c : ', forward_c.shape, forward_c)\n",
        "print('backward_h : ', backward_h.shape, backward_h)\n",
        "print('backward_c : ', backward_c.shape, backward_c)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLmti2QISYyV",
        "colab_type": "text"
      },
      "source": [
        "## Many-to-one"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLX9Rho9SbbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# shape = 要看幾個字,幾天的資料，資料向量長度\n",
        "# 設定output的向量長度(units)為128\n",
        "\n",
        "input = layers.Input(shape=(5, 8))\n",
        "x = layers.LSTM(128)(input)\n",
        "ouput = layers.Dense(10, activation='softmax')(x)\n",
        "model = models.Model(input, ouput)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT1W5B-2Td0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# multi-layer\n",
        "# 多搭幾層的LSTM\n",
        "# 只要進入下一層，必定要設定return_sequences=True，這樣才有對應的LSTM去做對接\n",
        "\n",
        "input = layers.Input(shape=(5, 8))\n",
        "x = layers.LSTM(128, return_sequences=True)(input)\n",
        "x = layers.LSTM(10)(x)\n",
        "ouput = layers.Dense(10)(x)\n",
        "model = models.Model(input, ouput)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNoebK2JTsMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgYNk_0dUVyE",
        "colab_type": "text"
      },
      "source": [
        "# Many-to-many (same length)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ThPhdZVUkRM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LSTM的output 跟目標分類結果可能不同，所以我們在每個時間點接一個Dense()\n",
        "\n",
        "input = layers.Input(shape=(50, 8))\n",
        "x = layers.LSTM(10, return_sequences=True)(input)\n",
        "ouput = layers.TimeDistributed(layers.Dense(20))(x)\n",
        "model = models.Model(input, ouput)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma9TzYCQWoh7",
        "colab_type": "text"
      },
      "source": [
        "## Many-to-many (different length) : Seq2seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGzcPsSkWuaW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}